{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "ea3f4874-a9aa-42f1-9605-b1784a6f48ba",
    "_uuid": "58c82d3b3c4b4305b388a6ac4eeca49d600f9105"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from os.path import join as opj\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from skimage.restoration import denoise_tv_bregman\n",
    "import pylab\n",
    "plt.rcParams['figure.figsize'] = 10, 10\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "804d3969-9035-4ceb-bb65-1b8549d729ec",
    "_uuid": "7a7f3af5ef279a9ed26c4d9ee764bd1fb4bdf10e"
   },
   "outputs": [],
   "source": [
    "#Load the data.\n",
    "train = pd.read_json(\"input/train.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "7b546aab-7b7d-4cde-91cc-e794fd4041bd",
    "_uuid": "2c18cf164fbbc6d1c29e9c668cbfcd7a1ea10824"
   },
   "outputs": [],
   "source": [
    "test = pd.read_json(\"input/test.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e178779f-0698-47cd-9be5-50f9c9590089",
    "_uuid": "f5b6c2ba24e6bf5726f8551cdeeeaf931184c2bc"
   },
   "source": [
    "#Intro about the Data.\n",
    "\n",
    "Sentinet -1 sat is at about 680 Km above earth. Sending pulses of signals at a particular angle of incidence and then recoding it back. Basically those reflected signals are called backscatter. The data we have been given is backscatter coefficient which is the conventional form of backscatter coefficient given by:\n",
    "\n",
    "$σo (dB) = βo (dB) + 10log10 [ sin(ip) / sin (ic)] $\n",
    "\n",
    "where\n",
    "1. ip=is angle of incidence for a particular pixel\n",
    "2. 'ic ' is angle of incidence for center of the image\n",
    "3. K =constant.\n",
    "\n",
    "We have been given $σo$ directly in the data. \n",
    "###Now coming to the features of $σo$\n",
    "Basically σo varies with the surface on which the signal is scattered from. For example, for a particular angle of incidence, it varies like:\n",
    "*             WATER...........           SETTLEMENTS........           AGRICULTURE...........          BARREN........\n",
    "\n",
    "1.**HH:**     -27.001   ................                     2.70252       .................                -12.7952        ................    -17.25790909\n",
    "\n",
    "2.**HV: **      -28.035      ................            -20.2665             ..................          -21.4471       .................     -20.019\n",
    "\n",
    "As you can see, the HH component varies a lot but HV doesn't.\n",
    "**I don't have the data for scatter from ship, but being a metal object, it should vary differently as compared to ice object.**\n",
    "\n",
    "###WTF is HH HV?\n",
    "\n",
    "Ok, so this Sentinal Settalite is equivalent to RISTSAT(an Indian remote sensing Sat) and they only Transmit pings in H polarization, **AND NOT IN V polarization**.  Those H-pings gets scattered, objects change their polarization and returns as a mix of H and V.\n",
    "**Since Sentinel has only H-transmitter, return signals are of the form of HH and HV only**. Don't ask why VV is not given(because Sentinel don't have V-ping transmitter).\n",
    "\n",
    "Now coming to features, for the purpose of this demo code, I am extracting all two bands and taking avg of them as 3rd channel to create a 3-channel RGB equivalent. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import rotate as rot\n",
    "def augment(images):\n",
    "    image_mirror_lr = []\n",
    "    image_mirror_ud = []\n",
    "    image_rotate30 = []\n",
    "    image_rotate60 = []\n",
    "    image_rotate110 = []\n",
    "    image_rotate155 = []\n",
    "    for i in range(0,images.shape[0]):\n",
    "        band_1 = images[i,:,:,0]\n",
    "        band_2 = images[i,:,:,1]\n",
    "            \n",
    "        # mirror left-right\n",
    "        band_1_mirror_lr = np.flip(band_1, 0)\n",
    "        band_2_mirror_lr = np.flip(band_2, 0)\n",
    "        image_mirror_lr.append(np.dstack((band_1_mirror_lr, band_2_mirror_lr)))\n",
    "        \n",
    "        # mirror up-down\n",
    "        band_1_mirror_ud = np.flip(band_1, 1)\n",
    "        band_2_mirror_ud = np.flip(band_2, 1)\n",
    "        image_mirror_ud.append(np.dstack((band_1_mirror_ud, band_2_mirror_ud)))\n",
    "        \n",
    "        #rotate \n",
    "        band_1_rotate = rot(band_1, 30, reshape=False, mode='reflect')\n",
    "        band_2_rotate = rot(band_2, 30, reshape=False, mode='reflect')\n",
    "        image_rotate30.append(np.dstack((band_1_rotate, band_2_rotate)))\n",
    "        \n",
    "        band_1_rotate = rot(band_1, 60, reshape=False, mode='reflect')\n",
    "        band_2_rotate = rot(band_2, 60, reshape=False, mode='reflect')\n",
    "        image_rotate60.append(np.dstack((band_1_rotate, band_2_rotate)))\n",
    "\n",
    "        band_1_rotate = rot(band_1, 110, reshape=False, mode='reflect')\n",
    "        band_2_rotate = rot(band_2, 110, reshape=False, mode='reflect')\n",
    "        image_rotate110.append(np.dstack((band_1_rotate, band_2_rotate)))\n",
    "        \n",
    "        band_1_rotate = rot(band_1, 155, reshape=False, mode='reflect')\n",
    "        band_2_rotate = rot(band_2, 155, reshape=False, mode='reflect')\n",
    "        image_rotate155.append(np.dstack((band_1_rotate, band_2_rotate)))\n",
    "        \n",
    "    mirrorlr = np.array(image_mirror_lr)\n",
    "    mirrorud = np.array(image_mirror_ud)\n",
    "    rotated30 = np.array(image_rotate30)\n",
    "    rotated60 = np.array(image_rotate60)\n",
    "    rotated110 = np.array(image_rotate110)\n",
    "    rotated155 = np.array(image_rotate155)\n",
    "    images = np.concatenate((images, mirrorlr, mirrorud, rotated30, rotated60, rotated110, rotated155))\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_band_1=np.array([denoise_tv_bregman(np.array(band).reshape(75, 75),weight=0.5) for band in train[\"band_1\"]])\n",
    "X_band_2=np.array([denoise_tv_bregman(np.array(band).reshape(75, 75),weight=0.5) for band in train[\"band_2\"]])\n",
    "\n",
    "X_band_1=np.array([np.array(band).astype(np.float32) for band in X_band_1])\n",
    "X_band_2=np.array([np.array(band).astype(np.float32) for band in X_band_2])\n",
    "\n",
    "X_band_1=np.array([band-band.mean() for band in X_band_1])\n",
    "X_band_2=np.array([band-band.mean() for band in X_band_2])\n",
    "\n",
    "X_band_1=np.array([band/band.std() for band in X_band_1])\n",
    "X_band_2=np.array([band/band.std() for band in X_band_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_band_1=np.array([band.reshape(75, 75) for band in X_band_1])\n",
    "#X_band_2=np.array([band.reshape(75, 75) for band in X_band_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train = np.concatenate([X_band_1[:, :, :, np.newaxis], X_band_2[:, :, :, np.newaxis],((X_band_1+X_band_2)/2)[:, :, :, np.newaxis]], axis=-1)\n",
    "X_train2 = np.concatenate([X_band_1[:, :, :, np.newaxis], X_band_2[:, :, :, np.newaxis]], axis=-1)\n",
    "target_train=train['is_iceberg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "1d690d4a-09ca-417c-8090-2aa417c514dd",
    "_uuid": "a883659e53709da950d04a4e5349c66d77a9422f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\envs\\tfgpu\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "X_train_cv, X_valid, y_train_cv, y_valid = train_test_split(X_train2, target_train, random_state=1, train_size=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "829bf7db-fab1-4a2d-9562-0a37c6390d2a",
    "_uuid": "5292632717f11cd01c135dfabfd3cda9318cc639"
   },
   "outputs": [],
   "source": [
    "#X_train_cv = augment(X_train_cv)\n",
    "#y_train_cv = np.concatenate((y_train_cv,y_train_cv, y_train_cv, y_train_cv,y_train_cv,y_train_cv,y_train_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAARUAAAESCAYAAADT60FaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nVIwxk/L/AUJSI00A5ikYAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x23dbf72da58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(200, figsize=(15, 15))\n",
    "ax = fig.add_subplot(3, 3, 1)\n",
    "ax.imshow(X_train2[14,:,:,0],cmap='Greys')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(4, figsize=(15, 15))\n",
    "ax = fig.add_subplot(1, 3, 1)\n",
    "image1 = np.array(train[\"band_1\"][8]).reshape(75, 75)\n",
    "ax.imshow(image1,cmap='Greys')\n",
    "ax = fig.add_subplot(1, 3, 2)\n",
    "ax.imshow(denoise_tv_bregman(image1,weight=0.1),cmap='Greys')\n",
    "ax = fig.add_subplot(1, 3, 3)\n",
    "ax.imshow(denoise_tv_bregman(image1,weight=0.5),cmap='Greys')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(200, figsize=(20, 200))\n",
    "j=1\n",
    "for i in range(0,100):\n",
    "    if(target_train[i]==1):\n",
    "        ax = fig.add_subplot(100, 11, j)\n",
    "        ax.imshow(X_train2[i,:,:,0],cmap='Greys')\n",
    "        j+=1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(200, figsize=(20, 200))\n",
    "j=1\n",
    "for i in range(0,100):\n",
    "    if(target_train[i]==0):\n",
    "        ax = fig.add_subplot(100, 11, j)\n",
    "        ax.imshow(X_train2[i,:,:,0],cmap='Greys')\n",
    "        j+=1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a95eedd5-fc75-4834-a817-e3ad700923f5",
    "_uuid": "01b69c50c6425d7d35b9bbefca7c06ea4bf1214b",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Take a look at a iceberg\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "py.init_notebook_mode(connected=True)\n",
    "def plotmy3d(c, name):\n",
    "\n",
    "    data = [\n",
    "        go.Surface(\n",
    "            z=c\n",
    "        )\n",
    "    ]\n",
    "    layout = go.Layout(\n",
    "        title=name,\n",
    "        autosize=False,\n",
    "        width=700,\n",
    "        height=700,\n",
    "        margin=dict(\n",
    "            l=65,\n",
    "            r=50,\n",
    "            b=65,\n",
    "            t=90\n",
    "        )\n",
    "    )\n",
    "    fig = go.Figure(data=data, layout=layout)\n",
    "    py.iplot(fig)\n",
    "plotmy3d(X_train2[1,:,:,0], 'iceberg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "cd7ea3c1-f039-445e-b5da-adfb608930d7",
    "_uuid": "1c65412c80ff504df19d55aa14093ebcd1028e6a"
   },
   "source": [
    "That's a cool looking iceberg we have. Remember, in radar data, the shape of the iceberg is going to be like a mountain as shown in here. Since this is not a actual image but scatter from radar, the shape is going to have peaks and distortions like these. The shape of the ship is going to be like a point, may be like a elongated point. From here the structural differences arise and we can exploit those differences using a CNN. It would be helpful if we can create composite images using the backscatter from radar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f78d43c6-c83e-47e3-a279-4dc8d3481c6c",
    "_uuid": "ab163fd947f3f108eacb0d367bf62505b0b9df9b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plotmy3d(X_train2[14,:,:,1], 'Ship')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1e2a6f33-992a-435e-be65-5ee614e6f7ba",
    "_uuid": "f907f993a1c63f4519872ecb381021c1a0dce2ca"
   },
   "source": [
    "That's a ship, looks like a elongated point. We don't have much resolution in images to visualize the shape of the ship. However CNN is here to help. There are few papers on ship iceberg classification like this:\n",
    "http://elib.dlr.de/99079/2/2016_BENTES_Frost_Velotto_Tings_EUSAR_FP.pdf\n",
    "However their data have much better resolution so I don't  feel that the CNN they used would be suitable here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "fb15bc53-becc-4e87-88ce-3bc99d45358d",
    "_uuid": "7a68a94f8c617209dfe56a58e291193e963d0f62"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Import Keras.\n",
    "from matplotlib import pyplot\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Input, Flatten, Activation\n",
    "from keras.layers import GlobalMaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.models import Model\n",
    "from keras import initializers\n",
    "from keras.optimizers import Adam\n",
    "from keras.optimizers import Nadam\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\n",
    "import keras.regularizers as reg\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_callbacks(filepath, patience=2):\n",
    "    es = EarlyStopping('val_loss', patience=patience, mode=\"min\")\n",
    "    msave = ModelCheckpoint(filepath, save_best_only=True)\n",
    "    return [es, msave]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getModel():\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(64, kernel_size=(3,3), input_shape=(75,75,2)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(128, kernel_size=(3, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(128, kernel_size=(2, 2)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(64, kernel_size=(2,2), input_shape=(75,75,2)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(256))\n",
    "    model.add(Activation('relu'))\n",
    "    #model.add(Dropout(0.7))\n",
    "    \n",
    "    model.add(Dense(256))\n",
    "    model.add(Activation('relu'))\n",
    "    #model.add(Dropout(0.7))\n",
    "    \n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "    mypotim=Adam(lr=0.00015)\n",
    "    #mypotim=SGD(lr=0.001,decay=1e-4, momentum=0.9,nesterov=True)\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer = mypotim, metrics=['accuracy'])\n",
    "\n",
    "    model.summary()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\envs\\tfgpu\\lib\\site-packages\\keras\\preprocessing\\image.py:653: UserWarning: Expected input to be images (as Numpy array) following the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3 or 4 channels on axis 3. However, it was passed an array with shape (1203, 75, 75, 2) (2 channels).\n",
      "  ' (' + str(x.shape[self.channel_axis]) + ' channels).')\n"
     ]
    }
   ],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.3,\n",
    "    height_shift_range=0.3,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    fill_mode='wrap')\n",
    "datagen.fit(X_train_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "_cell_guid": "d6bb750a-e882-4429-ad23-4392389f427f",
    "_uuid": "4e6dab11165b7d9515eb32b698851b260f0d941f",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_38 (Conv2D)           (None, 73, 73, 64)        1216      \n",
      "_________________________________________________________________\n",
      "activation_59 (Activation)   (None, 73, 73, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_32 (MaxPooling (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_39 (Conv2D)           (None, 34, 34, 128)       73856     \n",
      "_________________________________________________________________\n",
      "activation_60 (Activation)   (None, 34, 34, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_33 (MaxPooling (None, 17, 17, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_40 (Conv2D)           (None, 16, 16, 128)       65664     \n",
      "_________________________________________________________________\n",
      "activation_61 (Activation)   (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_34 (MaxPooling (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_41 (Conv2D)           (None, 7, 7, 64)          32832     \n",
      "_________________________________________________________________\n",
      "activation_62 (Activation)   (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_35 (MaxPooling (None, 3, 3, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 256)               147712    \n",
      "_________________________________________________________________\n",
      "activation_63 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "activation_64 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 1)                 257       \n",
      "_________________________________________________________________\n",
      "activation_65 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 387,329\n",
      "Trainable params: 387,329\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\envs\\tfgpu\\lib\\site-packages\\keras\\preprocessing\\image.py:787: UserWarning: NumpyArrayIterator is set to use the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3 or 4 channels on axis 3. However, it was passed an array with shape (1203, 75, 75, 2) (2 channels).\n",
      "  ' (' + str(self.x.shape[channels_axis]) + ' channels).')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "38/37 [==============================] - 2s - loss: 0.6728 - acc: 0.5777 - val_loss: 0.6406 - val_acc: 0.6035\n",
      "Epoch 2/200\n",
      "38/37 [==============================] - 1s - loss: 0.6365 - acc: 0.6350 - val_loss: 0.5846 - val_acc: 0.6658\n",
      "Epoch 3/200\n",
      "38/37 [==============================] - 1s - loss: 0.5780 - acc: 0.6948 - val_loss: 0.5175 - val_acc: 0.7581\n",
      "Epoch 4/200\n",
      "38/37 [==============================] - 1s - loss: 0.5415 - acc: 0.7376 - val_loss: 0.4762 - val_acc: 0.7980\n",
      "Epoch 5/200\n",
      "38/37 [==============================] - 1s - loss: 0.4864 - acc: 0.7749 - val_loss: 0.4016 - val_acc: 0.8354\n",
      "Epoch 6/200\n",
      "38/37 [==============================] - 1s - loss: 0.4940 - acc: 0.7606 - val_loss: 0.3748 - val_acc: 0.8653\n",
      "Epoch 7/200\n",
      "38/37 [==============================] - 1s - loss: 0.4497 - acc: 0.7908 - val_loss: 0.3383 - val_acc: 0.8703\n",
      "Epoch 8/200\n",
      "38/37 [==============================] - 1s - loss: 0.4084 - acc: 0.8248 - val_loss: 0.3056 - val_acc: 0.8653\n",
      "Epoch 9/200\n",
      "38/37 [==============================] - 1s - loss: 0.3794 - acc: 0.8423 - val_loss: 0.3129 - val_acc: 0.8454\n",
      "Epoch 10/200\n",
      "38/37 [==============================] - 1s - loss: 0.3760 - acc: 0.8284 - val_loss: 0.3837 - val_acc: 0.8329\n",
      "Epoch 11/200\n",
      "38/37 [==============================] - 1s - loss: 0.3651 - acc: 0.8338 - val_loss: 0.2588 - val_acc: 0.8853\n",
      "Epoch 12/200\n",
      "38/37 [==============================] - 1s - loss: 0.3242 - acc: 0.8582 - val_loss: 0.3314 - val_acc: 0.8529\n",
      "Epoch 13/200\n",
      "38/37 [==============================] - 1s - loss: 0.3544 - acc: 0.8396 - val_loss: 0.3287 - val_acc: 0.8479\n",
      "Epoch 14/200\n",
      "38/37 [==============================] - 1s - loss: 0.3425 - acc: 0.8426 - val_loss: 0.2563 - val_acc: 0.8928\n",
      "Epoch 15/200\n",
      "38/37 [==============================] - 1s - loss: 0.3383 - acc: 0.8410 - val_loss: 0.3200 - val_acc: 0.8579\n",
      "Epoch 16/200\n",
      "38/37 [==============================] - 1s - loss: 0.3413 - acc: 0.8418 - val_loss: 0.3180 - val_acc: 0.8354\n",
      "Epoch 17/200\n",
      "38/37 [==============================] - 1s - loss: 0.3269 - acc: 0.8486 - val_loss: 0.3003 - val_acc: 0.8703\n",
      "Epoch 18/200\n",
      "38/37 [==============================] - 1s - loss: 0.3896 - acc: 0.8114 - val_loss: 0.2881 - val_acc: 0.8753\n",
      "Epoch 19/200\n",
      "38/37 [==============================] - 1s - loss: 0.3234 - acc: 0.8596 - val_loss: 0.2556 - val_acc: 0.8753\n",
      "Epoch 20/200\n",
      "38/37 [==============================] - 1s - loss: 0.2968 - acc: 0.8687 - val_loss: 0.2363 - val_acc: 0.8853\n",
      "Epoch 21/200\n",
      "38/37 [==============================] - 1s - loss: 0.3087 - acc: 0.8563 - val_loss: 0.4006 - val_acc: 0.8204\n",
      "Epoch 22/200\n",
      "38/37 [==============================] - 1s - loss: 0.3174 - acc: 0.8563 - val_loss: 0.2515 - val_acc: 0.8853\n",
      "Epoch 23/200\n",
      "38/37 [==============================] - 1s - loss: 0.3247 - acc: 0.8580 - val_loss: 0.2554 - val_acc: 0.8728\n",
      "Epoch 24/200\n",
      "38/37 [==============================] - 1s - loss: 0.3199 - acc: 0.8552 - val_loss: 0.2418 - val_acc: 0.8928\n",
      "Epoch 25/200\n",
      "38/37 [==============================] - 1s - loss: 0.2935 - acc: 0.8774 - val_loss: 0.2295 - val_acc: 0.9027\n",
      "Epoch 26/200\n",
      "38/37 [==============================] - 1s - loss: 0.2887 - acc: 0.8621 - val_loss: 0.2432 - val_acc: 0.8928\n",
      "Epoch 27/200\n",
      "38/37 [==============================] - 1s - loss: 0.2834 - acc: 0.8794 - val_loss: 0.3319 - val_acc: 0.8628\n",
      "Epoch 28/200\n",
      "38/37 [==============================] - 1s - loss: 0.2825 - acc: 0.8832 - val_loss: 0.2532 - val_acc: 0.8903\n",
      "Epoch 29/200\n",
      "38/37 [==============================] - 1s - loss: 0.2630 - acc: 0.8903 - val_loss: 0.2241 - val_acc: 0.8978\n",
      "Epoch 30/200\n",
      "38/37 [==============================] - 1s - loss: 0.3034 - acc: 0.8654 - val_loss: 0.3020 - val_acc: 0.8728\n",
      "Epoch 31/200\n",
      "38/37 [==============================] - 1s - loss: 0.2929 - acc: 0.8714 - val_loss: 0.2614 - val_acc: 0.8803\n",
      "Epoch 32/200\n",
      "38/37 [==============================] - 1s - loss: 0.2975 - acc: 0.8703 - val_loss: 0.2559 - val_acc: 0.8853\n",
      "Epoch 33/200\n",
      "38/37 [==============================] - 1s - loss: 0.3027 - acc: 0.8755 - val_loss: 0.2624 - val_acc: 0.8853\n",
      "Epoch 34/200\n",
      "38/37 [==============================] - 1s - loss: 0.2778 - acc: 0.8835 - val_loss: 0.2192 - val_acc: 0.9027\n",
      "Epoch 35/200\n",
      "38/37 [==============================] - 1s - loss: 0.2809 - acc: 0.8744 - val_loss: 0.2394 - val_acc: 0.9027\n",
      "Epoch 36/200\n",
      "38/37 [==============================] - 1s - loss: 0.2768 - acc: 0.8769 - val_loss: 0.2673 - val_acc: 0.8803\n",
      "Epoch 37/200\n",
      "38/37 [==============================] - 1s - loss: 0.2883 - acc: 0.8761 - val_loss: 0.2236 - val_acc: 0.9002\n",
      "Epoch 38/200\n",
      "38/37 [==============================] - 1s - loss: 0.2726 - acc: 0.8813 - val_loss: 0.2800 - val_acc: 0.8778\n",
      "Epoch 39/200\n",
      "38/37 [==============================] - 1s - loss: 0.2922 - acc: 0.8739 - val_loss: 0.2158 - val_acc: 0.9102\n",
      "Epoch 40/200\n",
      "38/37 [==============================] - 1s - loss: 0.2864 - acc: 0.8700 - val_loss: 0.2552 - val_acc: 0.8878\n",
      "Epoch 41/200\n",
      "38/37 [==============================] - 1s - loss: 0.2844 - acc: 0.8818 - val_loss: 0.2283 - val_acc: 0.9002\n",
      "Epoch 42/200\n",
      "38/37 [==============================] - 1s - loss: 0.2732 - acc: 0.8747 - val_loss: 0.2914 - val_acc: 0.8678\n",
      "Epoch 43/200\n",
      "38/37 [==============================] - 1s - loss: 0.2716 - acc: 0.8854 - val_loss: 0.2476 - val_acc: 0.8953\n",
      "Epoch 44/200\n",
      "38/37 [==============================] - 1s - loss: 0.2953 - acc: 0.8752 - val_loss: 0.2361 - val_acc: 0.8978\n",
      "Epoch 45/200\n",
      "38/37 [==============================] - 1s - loss: 0.2636 - acc: 0.8709 - val_loss: 0.2249 - val_acc: 0.9002\n",
      "Epoch 46/200\n",
      "38/37 [==============================] - 1s - loss: 0.2677 - acc: 0.8870 - val_loss: 0.3810 - val_acc: 0.8479\n",
      "Epoch 47/200\n",
      "38/37 [==============================] - 1s - loss: 0.2499 - acc: 0.8824 - val_loss: 0.2132 - val_acc: 0.8978\n",
      "Epoch 48/200\n",
      "38/37 [==============================] - 1s - loss: 0.2721 - acc: 0.8865 - val_loss: 0.2734 - val_acc: 0.8803\n",
      "Epoch 49/200\n",
      "38/37 [==============================] - 1s - loss: 0.2602 - acc: 0.8933 - val_loss: 0.2222 - val_acc: 0.9027\n",
      "Epoch 50/200\n",
      "38/37 [==============================] - 1s - loss: 0.2667 - acc: 0.8763 - val_loss: 0.2948 - val_acc: 0.8853\n",
      "Epoch 51/200\n",
      "38/37 [==============================] - 1s - loss: 0.2730 - acc: 0.8829 - val_loss: 0.3318 - val_acc: 0.8603\n",
      "Epoch 52/200\n",
      "38/37 [==============================] - 1s - loss: 0.2603 - acc: 0.8837 - val_loss: 0.2396 - val_acc: 0.8878\n",
      "Epoch 53/200\n",
      "38/37 [==============================] - 1s - loss: 0.2672 - acc: 0.8898 - val_loss: 0.2063 - val_acc: 0.9152\n",
      "Epoch 54/200\n",
      "38/37 [==============================] - 1s - loss: 0.2827 - acc: 0.8835 - val_loss: 0.2184 - val_acc: 0.8978\n",
      "Epoch 55/200\n",
      "38/37 [==============================] - 1s - loss: 0.2662 - acc: 0.8917 - val_loss: 0.2559 - val_acc: 0.8828\n",
      "Epoch 56/200\n",
      "38/37 [==============================] - 1s - loss: 0.2536 - acc: 0.8848 - val_loss: 0.3014 - val_acc: 0.8778\n",
      "Epoch 57/200\n",
      "38/37 [==============================] - 1s - loss: 0.2538 - acc: 0.8892 - val_loss: 0.2167 - val_acc: 0.9102\n",
      "Epoch 58/200\n",
      "38/37 [==============================] - 1s - loss: 0.2608 - acc: 0.8879 - val_loss: 0.2155 - val_acc: 0.9027\n",
      "Epoch 59/200\n",
      "38/37 [==============================] - 1s - loss: 0.2598 - acc: 0.8983 - val_loss: 0.2096 - val_acc: 0.9152\n",
      "Epoch 60/200\n",
      "38/37 [==============================] - 1s - loss: 0.2599 - acc: 0.8848 - val_loss: 0.2162 - val_acc: 0.9052\n",
      "Epoch 61/200\n",
      "38/37 [==============================] - 1s - loss: 0.2644 - acc: 0.8887 - val_loss: 0.2089 - val_acc: 0.9102\n",
      "Epoch 62/200\n",
      "38/37 [==============================] - 1s - loss: 0.2684 - acc: 0.8815 - val_loss: 0.2183 - val_acc: 0.9077\n",
      "Epoch 63/200\n",
      "38/37 [==============================] - 1s - loss: 0.2540 - acc: 0.8887 - val_loss: 0.2107 - val_acc: 0.9102\n",
      "Epoch 64/200\n",
      "38/37 [==============================] - 1s - loss: 0.2731 - acc: 0.8903 - val_loss: 0.2121 - val_acc: 0.9102\n",
      "Epoch 65/200\n",
      "38/37 [==============================] - 1s - loss: 0.2640 - acc: 0.8944 - val_loss: 0.2223 - val_acc: 0.9052\n",
      "Epoch 66/200\n",
      "38/37 [==============================] - 1s - loss: 0.2571 - acc: 0.8980 - val_loss: 0.2374 - val_acc: 0.9002\n",
      "Epoch 67/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/37 [==============================] - 1s - loss: 0.2586 - acc: 0.8933 - val_loss: 0.2801 - val_acc: 0.8828\n",
      "Epoch 68/200\n",
      "38/37 [==============================] - 1s - loss: 0.2643 - acc: 0.8879 - val_loss: 0.2451 - val_acc: 0.9027\n",
      "Epoch 69/200\n",
      "38/37 [==============================] - 1s - loss: 0.2587 - acc: 0.8840 - val_loss: 0.2000 - val_acc: 0.9102\n",
      "Epoch 70/200\n",
      "38/37 [==============================] - 1s - loss: 0.2776 - acc: 0.8810 - val_loss: 0.2370 - val_acc: 0.8828\n",
      "Epoch 71/200\n",
      "38/37 [==============================] - 1s - loss: 0.2644 - acc: 0.8868 - val_loss: 0.2065 - val_acc: 0.9227\n",
      "Epoch 72/200\n",
      "38/37 [==============================] - 1s - loss: 0.2299 - acc: 0.9073 - val_loss: 0.2045 - val_acc: 0.9202\n",
      "Epoch 73/200\n",
      "38/37 [==============================] - 1s - loss: 0.2491 - acc: 0.9007 - val_loss: 0.2519 - val_acc: 0.8928\n",
      "Epoch 74/200\n",
      "38/37 [==============================] - 1s - loss: 0.2410 - acc: 0.8994 - val_loss: 0.1936 - val_acc: 0.9102\n",
      "Epoch 75/200\n",
      "38/37 [==============================] - 1s - loss: 0.2355 - acc: 0.9043 - val_loss: 0.1883 - val_acc: 0.9252\n",
      "Epoch 76/200\n",
      "38/37 [==============================] - 1s - loss: 0.2377 - acc: 0.9040 - val_loss: 0.1997 - val_acc: 0.9177\n",
      "Epoch 77/200\n",
      "38/37 [==============================] - 1s - loss: 0.2629 - acc: 0.8859 - val_loss: 0.2620 - val_acc: 0.8828\n",
      "Epoch 78/200\n",
      "38/37 [==============================] - 1s - loss: 0.2417 - acc: 0.9013 - val_loss: 0.1944 - val_acc: 0.9152\n",
      "Epoch 79/200\n",
      "38/37 [==============================] - 1s - loss: 0.2351 - acc: 0.8925 - val_loss: 0.2342 - val_acc: 0.8928\n",
      "Epoch 80/200\n",
      "38/37 [==============================] - 1s - loss: 0.2586 - acc: 0.8933 - val_loss: 0.3691 - val_acc: 0.8554\n",
      "Epoch 81/200\n",
      "38/37 [==============================] - 1s - loss: 0.2649 - acc: 0.8942 - val_loss: 0.2064 - val_acc: 0.9027\n",
      "Epoch 82/200\n",
      "38/37 [==============================] - 1s - loss: 0.2533 - acc: 0.8931 - val_loss: 0.2066 - val_acc: 0.9077\n",
      "Epoch 83/200\n",
      "38/37 [==============================] - 1s - loss: 0.2384 - acc: 0.8955 - val_loss: 0.1959 - val_acc: 0.9202\n",
      "Epoch 84/200\n",
      "38/37 [==============================] - 1s - loss: 0.2394 - acc: 0.9065 - val_loss: 0.1976 - val_acc: 0.9127\n",
      "Epoch 85/200\n",
      "38/37 [==============================] - 1s - loss: 0.2405 - acc: 0.8936 - val_loss: 0.1950 - val_acc: 0.9127\n",
      "Epoch 86/200\n",
      "38/37 [==============================] - 1s - loss: 0.2566 - acc: 0.8985 - val_loss: 0.2061 - val_acc: 0.9127\n",
      "Epoch 87/200\n",
      "38/37 [==============================] - 1s - loss: 0.2368 - acc: 0.9038 - val_loss: 0.2115 - val_acc: 0.9202\n",
      "Epoch 88/200\n",
      "38/37 [==============================] - 1s - loss: 0.2424 - acc: 0.8983 - val_loss: 0.2127 - val_acc: 0.9102\n",
      "Epoch 89/200\n",
      "38/37 [==============================] - 1s - loss: 0.2413 - acc: 0.8928 - val_loss: 0.2539 - val_acc: 0.8978\n",
      "Epoch 90/200\n",
      "38/37 [==============================] - 1s - loss: 0.2412 - acc: 0.8961 - val_loss: 0.2076 - val_acc: 0.9127\n",
      "Epoch 91/200\n",
      "38/37 [==============================] - 1s - loss: 0.2486 - acc: 0.8920 - val_loss: 0.1997 - val_acc: 0.9252\n",
      "Epoch 92/200\n",
      "38/37 [==============================] - 1s - loss: 0.2311 - acc: 0.9002 - val_loss: 0.1896 - val_acc: 0.9302\n",
      "Epoch 93/200\n",
      "38/37 [==============================] - 1s - loss: 0.2391 - acc: 0.8944 - val_loss: 0.2072 - val_acc: 0.9127\n",
      "Epoch 94/200\n",
      "38/37 [==============================] - 1s - loss: 0.2338 - acc: 0.8980 - val_loss: 0.1979 - val_acc: 0.9202\n",
      "Epoch 95/200\n",
      "38/37 [==============================] - 1s - loss: 0.2494 - acc: 0.8911 - val_loss: 0.2067 - val_acc: 0.9102\n",
      "Epoch 96/200\n",
      "38/37 [==============================] - 1s - loss: 0.2533 - acc: 0.8835 - val_loss: 0.2559 - val_acc: 0.8953\n",
      "Epoch 97/200\n",
      "38/37 [==============================] - 1s - loss: 0.2502 - acc: 0.8955 - val_loss: 0.2050 - val_acc: 0.9152\n",
      "Epoch 98/200\n",
      "38/37 [==============================] - 1s - loss: 0.2326 - acc: 0.9035 - val_loss: 0.2180 - val_acc: 0.9127\n",
      "Epoch 99/200\n",
      "38/37 [==============================] - 1s - loss: 0.2265 - acc: 0.9095 - val_loss: 0.2345 - val_acc: 0.9027\n",
      "Epoch 100/200\n",
      "38/37 [==============================] - 1s - loss: 0.2547 - acc: 0.9002 - val_loss: 0.2172 - val_acc: 0.9152\n",
      "Epoch 101/200\n",
      "38/37 [==============================] - 1s - loss: 0.2295 - acc: 0.9032 - val_loss: 0.1942 - val_acc: 0.9252\n",
      "Epoch 102/200\n",
      "38/37 [==============================] - 1s - loss: 0.2328 - acc: 0.8955 - val_loss: 0.2101 - val_acc: 0.9127\n",
      "Epoch 103/200\n",
      "38/37 [==============================] - 1s - loss: 0.2357 - acc: 0.9092 - val_loss: 0.2219 - val_acc: 0.9002\n",
      "Epoch 104/200\n",
      "38/37 [==============================] - 1s - loss: 0.2385 - acc: 0.8988 - val_loss: 0.2299 - val_acc: 0.8953\n",
      "Epoch 105/200\n",
      "38/37 [==============================] - 1s - loss: 0.2310 - acc: 0.9117 - val_loss: 0.1847 - val_acc: 0.9277\n",
      "Epoch 106/200\n",
      "38/37 [==============================] - 1s - loss: 0.2255 - acc: 0.9117 - val_loss: 0.1928 - val_acc: 0.9152\n",
      "Epoch 107/200\n",
      "38/37 [==============================] - 1s - loss: 0.2277 - acc: 0.9070 - val_loss: 0.2669 - val_acc: 0.8853\n",
      "Epoch 108/200\n",
      "38/37 [==============================] - 1s - loss: 0.2444 - acc: 0.9021 - val_loss: 0.1834 - val_acc: 0.9202\n",
      "Epoch 109/200\n",
      "38/37 [==============================] - 1s - loss: 0.2253 - acc: 0.9065 - val_loss: 0.2282 - val_acc: 0.9077\n",
      "Epoch 110/200\n",
      "38/37 [==============================] - 1s - loss: 0.2307 - acc: 0.8936 - val_loss: 0.1944 - val_acc: 0.9202\n",
      "Epoch 111/200\n",
      "38/37 [==============================] - 1s - loss: 0.2461 - acc: 0.8958 - val_loss: 0.1927 - val_acc: 0.9227\n",
      "Epoch 112/200\n",
      "38/37 [==============================] - 1s - loss: 0.2256 - acc: 0.9007 - val_loss: 0.1858 - val_acc: 0.9252\n",
      "Epoch 113/200\n",
      "38/37 [==============================] - 1s - loss: 0.2216 - acc: 0.9035 - val_loss: 0.1938 - val_acc: 0.9177\n",
      "Epoch 114/200\n",
      "38/37 [==============================] - 1s - loss: 0.2283 - acc: 0.9073 - val_loss: 0.1837 - val_acc: 0.9227\n",
      "Epoch 115/200\n",
      "38/37 [==============================] - 1s - loss: 0.2197 - acc: 0.9090 - val_loss: 0.2013 - val_acc: 0.9052\n",
      "Epoch 116/200\n",
      "38/37 [==============================] - 1s - loss: 0.2289 - acc: 0.9040 - val_loss: 0.1861 - val_acc: 0.9302\n",
      "Epoch 117/200\n",
      "38/37 [==============================] - 1s - loss: 0.2176 - acc: 0.9147 - val_loss: 0.2209 - val_acc: 0.9102\n",
      "Epoch 118/200\n",
      "38/37 [==============================] - 1s - loss: 0.2269 - acc: 0.9010 - val_loss: 0.1981 - val_acc: 0.9077\n",
      "Epoch 119/200\n",
      "38/37 [==============================] - 1s - loss: 0.2345 - acc: 0.8999 - val_loss: 0.1993 - val_acc: 0.9002\n",
      "Epoch 120/200\n",
      "38/37 [==============================] - 1s - loss: 0.2245 - acc: 0.9049 - val_loss: 0.2506 - val_acc: 0.8953\n",
      "Epoch 121/200\n",
      "38/37 [==============================] - 1s - loss: 0.2449 - acc: 0.8936 - val_loss: 0.2290 - val_acc: 0.9077\n",
      "Epoch 122/200\n",
      "38/37 [==============================] - 1s - loss: 0.2138 - acc: 0.9142 - val_loss: 0.2304 - val_acc: 0.9052\n",
      "Epoch 123/200\n",
      "38/37 [==============================] - 1s - loss: 0.2276 - acc: 0.9101 - val_loss: 0.2414 - val_acc: 0.8978\n",
      "Epoch 124/200\n",
      "38/37 [==============================] - 1s - loss: 0.2231 - acc: 0.9103 - val_loss: 0.1910 - val_acc: 0.9202\n",
      "Epoch 125/200\n",
      "38/37 [==============================] - 1s - loss: 0.2388 - acc: 0.9087 - val_loss: 0.2409 - val_acc: 0.9002\n",
      "Epoch 126/200\n",
      "38/37 [==============================] - 1s - loss: 0.2138 - acc: 0.9123 - val_loss: 0.1822 - val_acc: 0.9252\n",
      "Epoch 127/200\n",
      "38/37 [==============================] - 1s - loss: 0.2149 - acc: 0.9109 - val_loss: 0.1999 - val_acc: 0.9027\n",
      "Epoch 128/200\n",
      "38/37 [==============================] - 1s - loss: 0.2258 - acc: 0.8977 - val_loss: 0.1980 - val_acc: 0.9102\n",
      "Epoch 129/200\n",
      "38/37 [==============================] - 1s - loss: 0.2121 - acc: 0.9139 - val_loss: 0.1898 - val_acc: 0.9252\n",
      "Epoch 130/200\n",
      "38/37 [==============================] - 1s - loss: 0.2163 - acc: 0.9112 - val_loss: 0.2039 - val_acc: 0.9152\n",
      "Epoch 131/200\n",
      "38/37 [==============================] - 1s - loss: 0.2470 - acc: 0.8996 - val_loss: 0.2020 - val_acc: 0.9152\n",
      "Epoch 132/200\n",
      "38/37 [==============================] - 1s - loss: 0.2110 - acc: 0.9164 - val_loss: 0.1963 - val_acc: 0.9177\n",
      "Epoch 133/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/37 [==============================] - 1s - loss: 0.2275 - acc: 0.9007 - val_loss: 0.2250 - val_acc: 0.9077\n",
      "Epoch 134/200\n",
      "38/37 [==============================] - 1s - loss: 0.2318 - acc: 0.9073 - val_loss: 0.1816 - val_acc: 0.9377\n",
      "Epoch 135/200\n",
      "38/37 [==============================] - 1s - loss: 0.2239 - acc: 0.9027 - val_loss: 0.2003 - val_acc: 0.9102\n",
      "Epoch 136/200\n",
      "38/37 [==============================] - 1s - loss: 0.2221 - acc: 0.9095 - val_loss: 0.2227 - val_acc: 0.9002\n",
      "Epoch 137/200\n",
      "38/37 [==============================] - 1s - loss: 0.2093 - acc: 0.9246 - val_loss: 0.1936 - val_acc: 0.9252\n",
      "Epoch 138/200\n",
      "38/37 [==============================] - 1s - loss: 0.2142 - acc: 0.9092 - val_loss: 0.1966 - val_acc: 0.9127\n",
      "Epoch 139/200\n",
      "38/37 [==============================] - 1s - loss: 0.2111 - acc: 0.9114 - val_loss: 0.2040 - val_acc: 0.9152\n",
      "Epoch 140/200\n",
      "38/37 [==============================] - 1s - loss: 0.2071 - acc: 0.9147 - val_loss: 0.2238 - val_acc: 0.9077\n",
      "Epoch 141/200\n",
      "38/37 [==============================] - 1s - loss: 0.2613 - acc: 0.8843 - val_loss: 0.2816 - val_acc: 0.8628\n",
      "Epoch 142/200\n",
      "38/37 [==============================] - 1s - loss: 0.2329 - acc: 0.9021 - val_loss: 0.2736 - val_acc: 0.8853\n",
      "Epoch 143/200\n",
      "38/37 [==============================] - 1s - loss: 0.2256 - acc: 0.9114 - val_loss: 0.2546 - val_acc: 0.8853\n",
      "Epoch 144/200\n",
      "38/37 [==============================] - 1s - loss: 0.2056 - acc: 0.9090 - val_loss: 0.2476 - val_acc: 0.8978\n",
      "Epoch 145/200\n",
      "38/37 [==============================] - 1s - loss: 0.2043 - acc: 0.9051 - val_loss: 0.1843 - val_acc: 0.9302\n",
      "Epoch 146/200\n",
      "38/37 [==============================] - 1s - loss: 0.2139 - acc: 0.9054 - val_loss: 0.1990 - val_acc: 0.9052\n",
      "Epoch 147/200\n",
      "38/37 [==============================] - 1s - loss: 0.2194 - acc: 0.9060 - val_loss: 0.1772 - val_acc: 0.9327\n",
      "Epoch 148/200\n",
      "38/37 [==============================] - 1s - loss: 0.2062 - acc: 0.9101 - val_loss: 0.1715 - val_acc: 0.9302\n",
      "Epoch 149/200\n",
      "38/37 [==============================] - 1s - loss: 0.2117 - acc: 0.9109 - val_loss: 0.1803 - val_acc: 0.9252\n",
      "Epoch 150/200\n",
      "38/37 [==============================] - 1s - loss: 0.1987 - acc: 0.9172 - val_loss: 0.2230 - val_acc: 0.9002\n",
      "Epoch 151/200\n",
      "38/37 [==============================] - 1s - loss: 0.2130 - acc: 0.9016 - val_loss: 0.2532 - val_acc: 0.8953\n",
      "Epoch 152/200\n",
      "38/37 [==============================] - 1s - loss: 0.2219 - acc: 0.9103 - val_loss: 0.2162 - val_acc: 0.8978\n",
      "Epoch 153/200\n",
      "38/37 [==============================] - 1s - loss: 0.2184 - acc: 0.9112 - val_loss: 0.2420 - val_acc: 0.8978\n",
      "Epoch 154/200\n",
      "38/37 [==============================] - 1s - loss: 0.2001 - acc: 0.9123 - val_loss: 0.2166 - val_acc: 0.9102\n",
      "Epoch 155/200\n",
      "38/37 [==============================] - 1s - loss: 0.2037 - acc: 0.9106 - val_loss: 0.2144 - val_acc: 0.9102\n",
      "Epoch 156/200\n",
      "38/37 [==============================] - 1s - loss: 0.2141 - acc: 0.9016 - val_loss: 0.2043 - val_acc: 0.9102\n",
      "Epoch 157/200\n",
      "38/37 [==============================] - 1s - loss: 0.2006 - acc: 0.9090 - val_loss: 0.2252 - val_acc: 0.9077\n",
      "Epoch 158/200\n",
      "38/37 [==============================] - 1s - loss: 0.2082 - acc: 0.9112 - val_loss: 0.1774 - val_acc: 0.9352\n",
      "Epoch 159/200\n",
      "38/37 [==============================] - 1s - loss: 0.1911 - acc: 0.9197 - val_loss: 0.1929 - val_acc: 0.9252\n",
      "Epoch 160/200\n",
      "38/37 [==============================] - 1s - loss: 0.1968 - acc: 0.9142 - val_loss: 0.1828 - val_acc: 0.9277\n",
      "Epoch 161/200\n",
      "38/37 [==============================] - 1s - loss: 0.2271 - acc: 0.9112 - val_loss: 0.1945 - val_acc: 0.9302\n",
      "Epoch 162/200\n",
      "38/37 [==============================] - 1s - loss: 0.2100 - acc: 0.9065 - val_loss: 0.2087 - val_acc: 0.9102\n",
      "Epoch 163/200\n",
      "38/37 [==============================] - 1s - loss: 0.2159 - acc: 0.9073 - val_loss: 0.2139 - val_acc: 0.9027\n",
      "Epoch 164/200\n",
      "38/37 [==============================] - 1s - loss: 0.2213 - acc: 0.9016 - val_loss: 0.2196 - val_acc: 0.9052\n",
      "Epoch 165/200\n",
      "38/37 [==============================] - 1s - loss: 0.2198 - acc: 0.9177 - val_loss: 0.1897 - val_acc: 0.9277\n",
      "Epoch 166/200\n",
      "38/37 [==============================] - 1s - loss: 0.1994 - acc: 0.9216 - val_loss: 0.1932 - val_acc: 0.9127\n",
      "Epoch 167/200\n",
      "38/37 [==============================] - 1s - loss: 0.2020 - acc: 0.9169 - val_loss: 0.1899 - val_acc: 0.9152\n",
      "Epoch 168/200\n",
      "38/37 [==============================] - 1s - loss: 0.1938 - acc: 0.9230 - val_loss: 0.1863 - val_acc: 0.9152\n",
      "Epoch 169/200\n",
      "38/37 [==============================] - 1s - loss: 0.1959 - acc: 0.9221 - val_loss: 0.1874 - val_acc: 0.9177\n",
      "Epoch 170/200\n",
      "38/37 [==============================] - 1s - loss: 0.1998 - acc: 0.9120 - val_loss: 0.2237 - val_acc: 0.8903\n",
      "Epoch 171/200\n",
      "38/37 [==============================] - 1s - loss: 0.2122 - acc: 0.9095 - val_loss: 0.1887 - val_acc: 0.9252\n",
      "Epoch 172/200\n",
      "38/37 [==============================] - 1s - loss: 0.2204 - acc: 0.9087 - val_loss: 0.2075 - val_acc: 0.9102\n",
      "Epoch 173/200\n",
      "38/37 [==============================] - 1s - loss: 0.1873 - acc: 0.9213 - val_loss: 0.1810 - val_acc: 0.9177\n",
      "Epoch 174/200\n",
      "38/37 [==============================] - 1s - loss: 0.1758 - acc: 0.9287 - val_loss: 0.1835 - val_acc: 0.9277\n",
      "Epoch 175/200\n",
      "38/37 [==============================] - 1s - loss: 0.1927 - acc: 0.9282 - val_loss: 0.1864 - val_acc: 0.9227\n",
      "Epoch 176/200\n",
      "38/37 [==============================] - 1s - loss: 0.1866 - acc: 0.9232 - val_loss: 0.1758 - val_acc: 0.9327\n",
      "Epoch 177/200\n",
      "38/37 [==============================] - 1s - loss: 0.1988 - acc: 0.9131 - val_loss: 0.1803 - val_acc: 0.9227\n",
      "Epoch 178/200\n",
      "38/37 [==============================] - 1s - loss: 0.1985 - acc: 0.9169 - val_loss: 0.1888 - val_acc: 0.9227\n",
      "Epoch 179/200\n",
      "38/37 [==============================] - 1s - loss: 0.1845 - acc: 0.9284 - val_loss: 0.2035 - val_acc: 0.9002\n",
      "Epoch 180/200\n",
      "38/37 [==============================] - 1s - loss: 0.2166 - acc: 0.9084 - val_loss: 0.1789 - val_acc: 0.9202\n",
      "Epoch 181/200\n",
      "38/37 [==============================] - 1s - loss: 0.2130 - acc: 0.9136 - val_loss: 0.1860 - val_acc: 0.9352\n",
      "Epoch 182/200\n",
      "38/37 [==============================] - 1s - loss: 0.1987 - acc: 0.9169 - val_loss: 0.1849 - val_acc: 0.9252\n",
      "Epoch 183/200\n",
      "38/37 [==============================] - 1s - loss: 0.1970 - acc: 0.9224 - val_loss: 0.1779 - val_acc: 0.9302\n",
      "Epoch 184/200\n",
      "38/37 [==============================] - 1s - loss: 0.1881 - acc: 0.9235 - val_loss: 0.1900 - val_acc: 0.9327\n",
      "Epoch 185/200\n",
      "38/37 [==============================] - 1s - loss: 0.1897 - acc: 0.9246 - val_loss: 0.1915 - val_acc: 0.9327\n",
      "Epoch 186/200\n",
      "38/37 [==============================] - 1s - loss: 0.1898 - acc: 0.9301 - val_loss: 0.1895 - val_acc: 0.9202\n",
      "Epoch 187/200\n",
      "38/37 [==============================] - 1s - loss: 0.1780 - acc: 0.9320 - val_loss: 0.1840 - val_acc: 0.9252\n",
      "Epoch 188/200\n",
      "38/37 [==============================] - 1s - loss: 0.1827 - acc: 0.9221 - val_loss: 0.1993 - val_acc: 0.9277\n",
      "Epoch 189/200\n",
      "38/37 [==============================] - 1s - loss: 0.1716 - acc: 0.9279 - val_loss: 0.1963 - val_acc: 0.9152\n",
      "Epoch 190/200\n",
      "38/37 [==============================] - 1s - loss: 0.1778 - acc: 0.9268 - val_loss: 0.2020 - val_acc: 0.9102\n",
      "Epoch 191/200\n",
      "38/37 [==============================] - 1s - loss: 0.2220 - acc: 0.9098 - val_loss: 0.2569 - val_acc: 0.8853\n",
      "Epoch 192/200\n",
      "38/37 [==============================] - 1s - loss: 0.2280 - acc: 0.9046 - val_loss: 0.2128 - val_acc: 0.9227\n",
      "Epoch 193/200\n",
      "38/37 [==============================] - 1s - loss: 0.2019 - acc: 0.9177 - val_loss: 0.1831 - val_acc: 0.9252\n",
      "Epoch 194/200\n",
      "38/37 [==============================] - 1s - loss: 0.1946 - acc: 0.9166 - val_loss: 0.1889 - val_acc: 0.9302\n",
      "Epoch 195/200\n",
      "38/37 [==============================] - 1s - loss: 0.1756 - acc: 0.9350 - val_loss: 0.2010 - val_acc: 0.9102\n",
      "Epoch 196/200\n",
      "38/37 [==============================] - 1s - loss: 0.1985 - acc: 0.9134 - val_loss: 0.1884 - val_acc: 0.9152\n",
      "Epoch 197/200\n",
      "38/37 [==============================] - 1s - loss: 0.1685 - acc: 0.9301 - val_loss: 0.1888 - val_acc: 0.9102\n",
      "Epoch 198/200\n",
      "38/37 [==============================] - 1s - loss: 0.1674 - acc: 0.9405 - val_loss: 0.1954 - val_acc: 0.9177\n",
      "Epoch 199/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/37 [==============================] - 1s - loss: 0.1725 - acc: 0.9364 - val_loss: 0.1980 - val_acc: 0.9277\n",
      "Epoch 200/200\n",
      "38/37 [==============================] - 1s - loss: 0.1901 - acc: 0.9197 - val_loss: 0.2173 - val_acc: 0.9002\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x24068b505c0>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "gmodel=getModel()\n",
    "batch_size = 32\n",
    "ts = str(int(time.time()))\n",
    "file_path = \".model_weights_generator\"+ts+\".hdf5\"\n",
    "gmodel.fit_generator(datagen.flow(X_train_cv, y_train_cv, batch_size=batch_size),\n",
    "                    steps_per_epoch=len(X_train_cv) / batch_size,\n",
    "                    epochs=200,\n",
    "                    verbose=1,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    callbacks=get_callbacks(filepath=file_path, patience=200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "_cell_guid": "079f0a8d-d2a5-4154-b37f-b425333e4ada",
    "_uuid": "0fa65f37d198cd6301376f179d9de0ccc1d40db3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "401/401 [==============================] - 0s     \n",
      "Test loss: 0.165269682458\n",
      "Test accuracy: 0.932668329474\n",
      "1088/1203 [==========================>...] - ETA: 0sTrain loss: 0.179714526049\n",
      "Train accuracy: 0.932668329524\n"
     ]
    }
   ],
   "source": [
    "gmodel.load_weights(filepath=file_path)\n",
    "score = gmodel.evaluate(X_valid, y_valid, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "gmodel.load_weights(filepath=file_path)\n",
    "score = gmodel.evaluate(X_train_cv, y_train_cv, verbose=1)\n",
    "print('Train loss:', score[0])\n",
    "print('Train accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "_cell_guid": "7cae1458-a566-4714-8b80-0b23fe88509c",
    "_uuid": "27f021784da863a2ad960a96b9c7394f25521802"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8424/8424 [==============================] - 2s     \n"
     ]
    }
   ],
   "source": [
    "X_band_test_1=np.array([denoise_tv_bregman(np.array(band).reshape(75, 75),weight=0.5) for band in test[\"band_1\"]])\n",
    "X_band_test_2=np.array([denoise_tv_bregman(np.array(band).reshape(75, 75),weight=0.5) for band in test[\"band_2\"]])\n",
    "\n",
    "X_band_test_1=np.array([np.array(band).astype(np.float32) for band in X_band_test_1])\n",
    "X_band_test_2=np.array([np.array(band).astype(np.float32) for band in X_band_test_2])\n",
    "\n",
    "X_band_test_1=np.array([band-band.mean() for band in X_band_test_1])\n",
    "X_band_test_2=np.array([band-band.mean() for band in X_band_test_2])\n",
    "\n",
    "X_band_test_1=np.array([band/band.std() for band in X_band_test_1])\n",
    "X_band_test_2=np.array([band/band.std() for band in X_band_test_2])\n",
    "\n",
    "\n",
    "X_test = np.concatenate([X_band_test_1[:, :, :, np.newaxis], X_band_test_2[:, :, :, np.newaxis]], axis=-1)\n",
    "predicted_test=gmodel.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "_cell_guid": "da3618f6-6e0a-475c-a390-7e17f5406c1a",
    "_uuid": "b34412c33fe8250df3285867d9a13e4bd08e8c12"
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame()\n",
    "submission['id']=test['id']\n",
    "submission['is_iceberg']=predicted_test.reshape((predicted_test.shape[0]))\n",
    "submission.to_csv('sub.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a5140949-c867-43fd-baba-49e57bec44cf",
    "_uuid": "741f9696d91da6af29266fb199a6c2fb80d26dfe",
    "collapsed": true
   },
   "source": [
    "#### Conclusion\n",
    "To increase the score, I have tried Speckle filtering, Indicence angle normalization and other preprocessing and they don't seems to work.  You may try and see but for me they are not giving any good results.\n",
    "\n",
    "You can't be on top-10 using this kernel, so here is one beautiful peice of information. The test dataset contain 8000 images, We can exploit this. We can do pseudo labelling to increase the predictions. Here is the article related to that:\n",
    "https://towardsdatascience.com/simple-explanation-of-semi-supervised-learning-and-pseudo-labeling-c2218e8c769b\n",
    "\n",
    "Upvote if you liked this kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "637e3662-38ac-4fa6-8065-48c8105026a9",
    "_uuid": "962411dc0d6a00c1730bfd22767542210c36f751"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
